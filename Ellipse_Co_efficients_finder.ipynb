{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ellipse Co-efficients finder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dy59XawTeg1"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import re\n",
        "import time\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQOJYvWeUHS5"
      },
      "source": [
        "#Write the form of the equation inside comments after this block    \n",
        "\n",
        "##Like this:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# I want the equation of the form:\n",
        "# y = w1*x + w2\n",
        "\n",
        "# Here, we have data for\n",
        "# 1. x\n",
        "# 2. y\n",
        "\n",
        "# Note that x and y are actually arrays because we have a lot of data\n",
        "# And, we want to find the co-efficients w1 and w2\n",
        "```\n",
        "\n",
        "#Start the comments here:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYEXykkhT7z6"
      },
      "source": [
        "# I want the equation of the form:\n",
        "# p = w9[w1a^w2 + w3b^w4 + w5a^w6b^w7 + w8]\n",
        "\n",
        "# Here, we have data for\n",
        "# 1. a\n",
        "# 2. b\n",
        "# 3. p\n",
        "\n",
        "# Note that a, b and p are actually arrays because we have a lot of data\n",
        "# And, we want to find the co-efficients w1, w2, w3, w4 and w5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfMMUcIL6n51"
      },
      "source": [
        "#Import or create data after this block\n",
        "\n",
        "##Like this:\n",
        "\n",
        "\n",
        "```\n",
        "m = np.random.rand(50,1)\n",
        "# print(m)\n",
        "n = 3*m + 6\n",
        "n = n.reshape((50,1))\n",
        "# print(n)\n",
        "```\n",
        "\n",
        "#Import or create your own data here:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cb_eVhGICcPE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "46d5c7ba-0c29-4d13-dee7-7615fc41c335"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/Ellipse data.csv')\n",
        "a = df.a\n",
        "b = df.b\n",
        "p = df.Perimeter\n",
        "a = np.array(a)\n",
        "b = np.array(b)\n",
        "p = np.array(p)\n",
        "print(a)\n",
        "\n",
        "print(b)\n",
        "\n",
        "print(p)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[80 39 51 ... 68 94 99]\n",
            "[99 68 54 ...  7 54 52]\n",
            "[563.93016296 342.35237332 329.9345519  ... 276.56586834 473.48599615\n",
            " 485.94151292]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1Lrh58ZgjiD"
      },
      "source": [
        "# x = tf.convert_to_tensor(a, dtype=np.float32)\n",
        "# y = tf.convert_to_tensor(b, dtype=np.float32)\n",
        "# c = tf.convert_to_tensor(p, dtype=np.float32)\n",
        "\n",
        "# w1 = tf.Variable(1, dtype='float32') \n",
        "# w2 = tf.Variable(1.1, dtype='float32')\n",
        "# w3 = tf.Variable(3, dtype='float32')\n",
        "# w4 = tf.Variable(1.1, dtype='float32')\n",
        "# w5 = tf.Variable(5, dtype='float32')\n",
        "# w6 = tf.Variable(1.1, dtype='float32')\n",
        "# w7 = tf.Variable(1.1, dtype='float32')\n",
        "# w8 = tf.Variable(8, dtype='float32')\n",
        "# w9 = tf.Variable(9, dtype='float32')\n",
        "\n",
        "# c_p = tf.math.multiply(tf.add(tf.add(tf.math.multiply(w1, tf.math.pow(x, tf.abs(w2))), tf.math.multiply(w3, tf.math.pow(y, tf.abs(w4)))), tf.add(tf.math.multiply(tf.math.multiply(tf.math.pow(x, tf.abs(w6)), tf.math.pow(y, tf.abs(w7))), w5), w8)), w9)\n",
        "# errors = c_p - c\n",
        "\n",
        "# mse = tf.reduce_mean(tf.math.multiply(errors, errors))\n",
        "\n",
        "\n",
        "# print(mse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIxR7bcmV5pB"
      },
      "source": [
        "#Code up the loss function after this block    \n",
        "\n",
        "##Like this:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "def loss_fun(x, y, w1, w2):\n",
        "  \n",
        "  # We don't need to use tf functions here\n",
        "  # The inputs x and y are arrays\n",
        "  y_pred = w1*x + w2\n",
        "  error = y_pred - y\n",
        "\n",
        "  # Now, we use a tf function called reduce_mean\n",
        "  # We are calculating Root Mean Square error\n",
        "  # This does NOT mean that we have used RMSprop optimizer\n",
        "  # This is just the loss function\n",
        "  # Optimizers part will come later\n",
        "  mse = tf.reduce_mean(errors**2)\n",
        "  return mse\n",
        "```\n",
        "\n",
        "#Start writing the loss function here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVAY7AltUR9i"
      },
      "source": [
        "def loss_fun(x, y, c, w1, w2, w3, w4, w5, w6, w7, w8, w9):\n",
        "  \n",
        "  # hi = tf.math.multiply(w1, tf.add(x, y)) - tf.sqrt(tf.math.abs(tf.math.multiply(tf.add(tf.math.multiply(w2, x), tf.math.multiply(w3, y)), tf.add(tf.math.multiply(w4, x), tf.math.multiply(w5, y)))))\n",
        "  # c_p = tf.math.multiply(3.1415, hi)\n",
        "\n",
        "  # c_p = tf.math.multiply(tf.add(tf.add(tf.math.multiply(w1, tf.math.pow(x, tf.abs(w2))), tf.math.multiply(w3, tf.math.pow(y, tf.abs(w4)))), tf.add(tf.math.multiply(tf.math.multiply(tf.math.pow(x, tf.abs(w6)), tf.math.pow(y, tf.abs(w7))), w5), w8)), w9)\n",
        "  # errors = c_p - c\n",
        "\n",
        "  c_p = math.pi*(w1*a + w2*b + w3*a*b + (w4*a + w5*b + w6*a*b)**w7)/(w8*a + w9*b + 1)\n",
        "  errors = c_p - c\n",
        "\n",
        "\n",
        "  mse = tf.reduce_mean(tf.math.multiply(errors, errors))\n",
        "\n",
        "  return mse\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f12M55KQZop8"
      },
      "source": [
        "#Code a Gradient Descent loop after this block    \n",
        "\n",
        "##Like this:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "def run_gradient_descent(x, y, init_w1, init_w2, learning_rate, iterations):\n",
        "     \n",
        "    tf_w1 = tf.Variable(init_w1, dtype='float32') \n",
        "    tf_w2 = tf.Variable(init_w2, dtype='float32')\n",
        "    \n",
        "    # Group trainable parameters into a list\n",
        "    trainable_params = [tf_w1, tf_w2]\n",
        "    \n",
        "    # Define your optimizer - Adam or RMSprop here\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "    # For RMSprop, it should be like this:-\n",
        "    # optimizer = tf.keras.optimizers.RMSprop(learning_rate)\n",
        "       \n",
        "    print(\"Started working\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    for i in range(iterations):\n",
        "        with tf.GradientTape() as tape:\n",
        "            \n",
        "            tape.watch(trainable_params)\n",
        "\n",
        "            loss = loss_fun(x, y, tf_w1, tf_w2)\n",
        "\n",
        "        # We can use the trainable parameters list directly in gradient calcs\n",
        "        gradients = tape.gradient(loss, trainable_params)\n",
        "\n",
        "        # Optimizers always aim to *minimize* the loss function\n",
        "        optimizer.apply_gradients(zip(gradients, trainable_params))\n",
        "        w1_val = re.findall('numpy=(.+)>',str(tf_w1))\n",
        "        w2_val = re.findall('numpy=(.+)>',str(tf_w2))\n",
        "\n",
        "        seconds = time.time() - start_time\n",
        "        print(str(\"\\rw1 = \" + str(w1_val[0]) + \"\\tw2 = \" + str(w2_val[0]) + \"\\t\\tIteration = \" + str(i) + \"\\t\\tProgress = \" + str(int((i/iterations)*100))+\" %\" + \"\\t\\tTime = \" + str(int(seconds)) + \"s\" ), end='', flush=True)\n",
        "\n",
        "    print(str(\"\\rw1 = \" + str(w1_val[0]) + \"\\tw2 = \" + str(w2_val[0]) + \"\\t\\tIteration = \" + str(iterations) + \"\\tProgress = 100 %\" + \"\\tTime = \" + str(int(seconds)) ), end='', flush=True)\n",
        "    print(\"\\nDone!\")\n",
        "```\n",
        "\n",
        "#Start coding your own Gradient Descent Runner here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTsl3AImXfjd"
      },
      "source": [
        "def run_gradient_descent(x, y, c, init_w1, init_w2, init_w3, init_w4, init_w5, init_w6, init_w7, init_w8, init_w9, learning_rate, iterations):\n",
        "     \n",
        "    tf_w1 = tf.Variable(init_w1, dtype='float32') \n",
        "    tf_w2 = tf.Variable(init_w2, dtype='float32')\n",
        "    tf_w3 = tf.Variable(init_w3, dtype='float32')\n",
        "    tf_w4 = tf.Variable(init_w4, dtype='float32')\n",
        "    tf_w5 = tf.Variable(init_w5, dtype='float32')\n",
        "    tf_w6 = tf.Variable(init_w6, dtype='float32')\n",
        "    tf_w7 = tf.Variable(init_w7, dtype='float32')\n",
        "    tf_w8 = tf.Variable(init_w8, dtype='float32')\n",
        "    tf_w9 = tf.Variable(init_w9, dtype='float32')\n",
        "    \n",
        "    # Group trainable parameters into a list\n",
        "    trainable_params = [tf_w1, tf_w2, tf_w3, tf_w4, tf_w5, tf_w6, tf_w7, tf_w8, tf_w9]\n",
        "    \n",
        "    # Define your optimizer - Adam or RMSprop here\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "    # For RMSprop, it should be like this:-\n",
        "    # optimizer = tf.keras.optimizers.RMSprop(learning_rate)\n",
        "       \n",
        "    print(\"Started working\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    x = tf.convert_to_tensor(x, dtype=np.float32)\n",
        "    y = tf.convert_to_tensor(y, dtype=np.float32)\n",
        "    c = tf.convert_to_tensor(c, dtype=np.float32)\n",
        "\n",
        "    for i in range(iterations):\n",
        "        with tf.GradientTape() as tape:\n",
        "            \n",
        "            tape.watch(trainable_params)\n",
        "\n",
        "            # hi = tf.math.multiply(tf_w1, tf.add(x, y)) - tf.sqrt(tf.math.abs(tf.math.multiply(tf.add(tf.math.multiply(tf_w2, x), tf.math.multiply(tf_w3, y)), tf.add(tf.math.multiply(tf_w4, x), tf.math.multiply(tf_w5, y)))))\n",
        "            # c_p = tf.math.multiply(3.1415, hi)\n",
        "\n",
        "            # errors = c_p - c\n",
        "            # loss = tf.reduce_mean(tf.math.multiply(errors, errors))\n",
        "            loss = loss_fun(x, y, c, tf_w1, tf_w2, tf_w3, tf_w4, tf_w5, tf_w6, tf_w7, tf_w8, tf_w9)\n",
        "            # print(loss)\n",
        "\n",
        "        # We can use the trainable parameters list directly in gradient calcs\n",
        "        gradients = tape.gradient(loss, trainable_params)\n",
        "\n",
        "        # Optimizers always aim to *minimize* the loss function\n",
        "        optimizer.apply_gradients(zip(gradients, trainable_params))\n",
        "        # w1_val = str(tf_w1)\n",
        "        w1_val = re.findall('numpy=(.+)>',str(tf_w1))\n",
        "        w2_val = re.findall('numpy=(.+)>',str(tf_w2))\n",
        "        w3_val = re.findall('numpy=(.+)>',str(tf_w3))\n",
        "        w4_val = re.findall('numpy=(.+)>',str(tf_w4))\n",
        "        w5_val = re.findall('numpy=(.+)>',str(tf_w5))\n",
        "        w6_val = re.findall('numpy=(.+)>',str(tf_w6))\n",
        "        w7_val = re.findall('numpy=(.+)>',str(tf_w7))\n",
        "        w8_val = re.findall('numpy=(.+)>',str(tf_w8))\n",
        "        w9_val = re.findall('numpy=(.+)>',str(tf_w9))\n",
        "        loss_val = re.findall('Tensor\\((.+), shape', str(loss))\n",
        "\n",
        "        seconds = time.time() - start_time\n",
        "        # tf.print(tf_w1)\n",
        "        print(str(\"\\rw1 = \" + str(w1_val[0]) + \" w2 = \" + str(w2_val[0]) + \" w3 = \" + str(w3_val[0]) + \" w4 = \" + str(w4_val[0]) + \" w5 = \" + str(w5_val[0]) + \" w6 = \" + str(w6_val[0]) + \" w7 = \" + str(w7_val[0]) + \" w8 = \" + str(w8_val[0]) + \" w9 = \" + str(w9_val[0]) + \"\\tLoss = \" + str(loss_val[0]) +\"\\tProgress = \" + str(int((i/iterations)*100))+\" %\" + \"\\tTime = \" + str(int(seconds)) + \"s\" ), end='', flush=True)\n",
        "\n",
        "    print(str(\"\\rw1 = \" + str(w1_val[0]) + \" w2 = \" + str(w2_val[0]) + \" w3 = \" + str(w3_val[0]) + \" w4 = \" + str(w4_val[0]) + \" w5 = \" + str(w5_val[0]) + \" w6 = \" + str(w6_val[0]) + \" w7 = \" + str(w7_val[0]) + \" w8 = \" + str(w8_val[0]) + \" w9 = \" + str(w9_val[0]) + \"\\tLoss = \" + str(loss_val[0]) +\"\\tIteration = \" + str(iterations) + \"\\tProgress = 100 %\" + \"\\tTime = \" + str(int(seconds)) + \"s\" ), end='', flush=True)\n",
        "    print(\"\\nDone!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUZ46qkxdcZy"
      },
      "source": [
        "#Call the Gradient Descent Runner after this block:    \n",
        "\n",
        "##Like this:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "run_gradient_descent(m, n, 4, 5, 0.01, 5000)\n",
        "```\n",
        "\n",
        "###Note that if you use Adam optimizer, you should keep the number of iterations a bit high. It jumps a bit in the starting\n",
        "\n",
        "#Call the Gradient Descent Runner here:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qxt2IEVbz_P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "360ddbb7-62a9-4122-8ca1-36a0c6ab9664"
      },
      "source": [
        "run_gradient_descent(a, b, p, 100, 100, 11, 8, 8, 9, -5.1, 4.2, 4.3, 0.1, 35000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Started working\n",
            "w1 = 27.776466 w2 = 26.99165 w3 = 1.4449896 w4 = 8.159543 w5 = 8.164167 w6 = 9.164316 w7 = -7.2608824 w8 = 0.54550636 w9 = 0.5189079\tLoss = 4234.6978\tIteration = 35000\tProgress = 100 %\tTime = 396s\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phArEpb2DsvU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}